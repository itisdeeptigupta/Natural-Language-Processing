---
title: 'Processing Raw Text Assignment'
author: "Deepti Gupta"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
#do not change this
knitr::opts_chunk$set(echo = TRUE)
```

In each step, you will process your data for common text data issues. Be sure to complete each one in *R* and Python separately - creating a clean text version in each language for comparison at the end. Update the saved clean text at each step, do not simply just print it out. 

## Libraries / R Setup

- In this section, include the libraries you need for the *R* questions.  

```{r}
##r chunk
library(reticulate)
library(rvest)
library(stringi)
library(textclean)
library(stringr)
library(tokenizers)
library(hunspell)
library(textstem)
library(tm)
py_config()


```

- In this section, include import functions to load the packages you will use for Python.

```{python}
##python chunk

import requests
#py_install("contractions", pip = T)

import contractions
import spacy

import unicodedata
import bs4

import re

import nltk

from nltk.corpus import stopwords
from contractions import contractions_dict

from textblob import Word

```

## Import data to work with

- Use `rvest` to import a webpage and process that text for `html` codes (i.e. take them out)!

```{r}
##r chunk

#blog_url = "https://onezero.medium.com/how-science-fiction-imagined-the-2020s-f8e98a5bc729"


blog_url = "https://www.aggieerin.com/post/getting-translations-with-rvest-and-selenium/"
blog_post = read_html(blog_url)

clean_text = html_text(blog_post)
clean_text

```

- Use the `requests` package to import the same webpage and use `BeautifulSoup` to clean up the `html` codes.

```{python}
##python chunk

#blog_post = requests.get("https://onezero.medium.com/how-science-fiction-imagined-the-2020s-f8e98a5bc729")

blog_post = requests.get("https://www.aggieerin.com/post/getting-translations-with-rvest-and-selenium/")
content = blog_post.content
print(content[1000:2000])

```

## Lower case

- Lower case the text you created using *R*.

```{r}
##r chunk

tolower(clean_text)

```

- Lower case the text you created using python.

```{python}
##python chunk

content.lower()

```

## Removing symbols

- Use the `stringi` package to remove any symbols from your text. 

```{r}
##r chunk

clean_text_1 = stri_trans_general(str = clean_text,
                   id = "Latin-ASCII")
clean_text_1

```

- Use the `unicodedata` in python to remove any symbols from your text. 

```{python}
##python chunk

def remove_accented_chars(text):
  text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')
  return text

clean_text_2 = remove_accented_chars(r.clean_text)
content_1 = remove_accented_chars(str(content))

clean_text_2

content_1

```

## Contractions

- Replace all the contractions in your webpage using *R*.

```{r}
##r chunk

words = unlist(tokenize_words(clean_text))
words[282]

words = str_replace_all(words, pattern = "’", 
                        replacement = "'")

words[282]

replace_contraction(words, 
                    contraction.key = lexicon::key_contractions, #default
                    ignore.case = T)[282] #default



```

- Replace all the contractions in your webpage using python.

```{python}
##python chunk

contractions_re = re.compile('|'.join(contractions_dict.keys()))

def expand_contractions(s, contractions_dict=contractions_dict):
  def replace(match):
    return contractions_dict[match.group(0)]
  return contractions_re.sub(replace, s)


expand_contractions(clean_text_2)


```
  
## Spelling

- Fix any spelling errors with the `hunspell` package in *R* - it's ok to use the first, most probable option, like we did in class. 

```{r}
##r chunk

wordlist = c("thse", "wods", "mispelled", "fairr", "laughng")

# Spell check the words
spelling.errors <- hunspell(wordlist)
spelling.sugg <- hunspell_suggest(unlist(spelling.errors), dict = dictionary("en_US"))

# Pick the first suggestion
spelling.sugg <- unlist(lapply(spelling.sugg, function(x) x[1]))
spelling.dict <- as.data.frame(cbind(spelling.errors,spelling.sugg))
spelling.dict$spelling.pattern <- paste0("\\b", spelling.dict$spelling.errors, "\\b")

#Replace the words 
stri_replace_all_regex(str = wordlist,
                       pattern = spelling.dict$spelling.pattern,
                       replacement = spelling.dict$spelling.sugg,
                       vectorize_all = FALSE)



```

- Fix your spelling errors using `textblob` from python. 

```{python}
##python chunk

wordlist = ["thse", "wods", "mispelled","fairr", "laughng"]

[Word(word).correct() for word in wordlist]

```

## Lemmatization

- Lemmatize your data in *R* using `textstem`. 

```{r}
##r chunk

wordlist = c("worded", "words", "word", "fairer", "fairy", "fair", "fairing")
lemmatize_words(wordlist)

sentence = "My system keeps crashing his crashed yesterday, ours crashes daily"
lemmatize_strings(sentence)

```

- Lemmatize your data in python using `spacy`. 

```{python}
##python chunk

nlp = spacy.load("C:\Program Files\Python38\Lib\site-packages\en_core_web_sm\en_core_web_sm-2.2.0")

def lemmatize_text(text):
  text = nlp(text)
  text = " ".join([word.lemma_ if word.lemma_ != "-PRON-" else word.text for word in text])
  return text

lemmatize_text(r.sentence)

sentence_2 = "worded words word fairer fairy fair fairing"
lemmatize_text(sentence_2)

```

## Stopwords

- Remove all the stopwords from your *R* clean text. 

```{r}
##r chunk

head(stopwords(kind = "smart"))
removeWords(sentence, stopwords(kind = "smart"))

```

- Remove all the stop words from your python clean text. 

```{python}
##python chunk

set(stopwords.words('english'))

[word for word in nltk.word_tokenize(r.sentence) if word not in stopwords.words('english')]

```

## Tokenization 

- Use the `tokenize_words` function to create a set of words for your *R* clean text. 

```{r}
##r chunk

tokenize_sentences(clean_text_1)[[1]][3:8]

count_sentences(clean_text_1) # hmmm
temp = unlist(tokenize_sentences(clean_text_1))
length(temp)

```

- Use `nltk` or `spacy` to tokenize the words from your python clean text. 

```{python}
##python chunk

sentences = nltk.sent_tokenize(r.clean_text_1)
print(sentences)
len(sentences)

```

## Check out the results

- Print out the first 100 tokens of your clean text from *R*. 

```{r}
##r chunk

tkn_cleantext = tokenize_words(clean_text,
               lowercase = T,
               stopwords = NULL, #more on this later
               strip_punct = T,
               strip_numeric = F,
               simplify = F) #list format

head(tkn_cleantext[[1]],100)
```

- Print out the first 100 tokens of your clean text from python. 

```{python}
##python chunk
words = nltk.word_tokenize(r.clean_text)
print(words[:100])

```

Note: here you can print out, summarize, or otherwise view your text in anyway you want. 

- ANSWER THIS: Compare the results from your processing. Write a short paragraph answering the following questions. You will need to write more than a few sentences for credit. 

  - Which text appears to be "cleaner"? 
  
      Ans. Since algorithms for the text cleaning in R and python work differently for different approaches, I found that text from R appears to be cleaner than in python but python offers other robust libraries to clean it further if needed which R might not have.
  
  
  - Or are they the same? 
  
      Ans. The results are almost same for Removing symbols and spelling but not for all the approaches.
    
    
  - What differences can you spot? 
  
      Ans. 
          1. Removing symbols : Under this part, R replaced © with (C) which is unneccessary but python removed this symbol completely
          2. Contractions: R worked better as it replaced I'll with I will but python failed and converted I'll to Ill
          3. Spelling: From the given wordlist which was processed in both the environments, python appeared to have a most accurate response
          4. Stopwords: A list of stopwords evaluated by Python appears more comprehensive than by R which has compiled a huge list and may lead to falling out of important words
          5. Tokenization: Tokenized sentences and words from R seems to be much cleaner and relatable but python is bringing back a lot of special characters and html code along with it. 
    
    
  - Which processing approach appears to be easier? 
  
      Ans. It would be wise not to stick on just one approach going forward and use both of them based on which one is yielding better results
